{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn as skl\n",
    "import math\n",
    "import random\n",
    "from scipy import optimize as op\n",
    "from random import randint\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# HW 5 Index analyses\n",
    "# Computes Gain info\n",
    "def get_info_D(n, feature):\n",
    "    info_D = 0\n",
    "    feature_counts = Counter(feature)\n",
    "    for val in feature_counts:\n",
    "        p_i = feature_counts[val]/n\n",
    "        info_D -= p_i*math.log(p_i, 2)\n",
    "    return info_D\n",
    "\n",
    "# Computes Gini value\n",
    "def get_gini_D(n, feature):\n",
    "    gini_D = 1\n",
    "    feature_counts = Counter(feature)\n",
    "    for val in feature_counts:\n",
    "        p_i = feature_counts[val]/n\n",
    "        gini_D -= math.pow(p_i, 2)\n",
    "    return gini_D\n",
    "\n",
    "# Returns info gains, feature split info, and gini indicies\n",
    "def get_info_gains(n, df):\n",
    "    # Info(D) and Gini(D)\n",
    "    class_data = df[df.columns[-1]]\n",
    "    info_D = get_info_D(n, class_data)\n",
    "    gini_D = get_gini_D(n, class_data)\n",
    "    \n",
    "    # Info_class(D), Split_Info(D), and Gini_class(D)\n",
    "    feature_info_gain = []\n",
    "    feature_split_info = []\n",
    "    feature_gini_indicies = []\n",
    "    \n",
    "    # Loop over attributes\n",
    "    for i in range(len(df.columns)-1):\n",
    "        # Create summation bases\n",
    "        feature_info = 0\n",
    "        split_info = 0\n",
    "        feature_gini = 0\n",
    "        \n",
    "        # Group by subsections\n",
    "        col = df.groupby(df[df.columns[i]])[df.columns[-1]]\n",
    "        for key in col: # key[0] = key, key[1] = table of key vs class\n",
    "            # Extract number of elements in subsection\n",
    "            num_keys = len(key[1])\n",
    "            \n",
    "            # Calculate respective info\n",
    "            split_info -= num_keys/n * math.log(num_keys/n, 2)\n",
    "            feature_info += num_keys/n * (get_info_D(num_keys, key[1]))\n",
    "            feature_gini += num_keys/n * (get_gini_D(num_keys, key[1]))\n",
    "            \n",
    "        # Append into respective array index\n",
    "        feature_info_gain.append(feature_info)\n",
    "        feature_split_info.append(split_info)\n",
    "        feature_gini_indicies.append(feature_gini)\n",
    "        \n",
    "    # Gain(class), \n",
    "    Gain_D = [info_D - info_A_D for info_A_D in feature_info_gain]\n",
    "    delta_gini_D = [gini_D - gini_A_D for gini_A_D in feature_gini_indicies]\n",
    "    \n",
    "    # Return attribute info arrays\n",
    "    return Gain_D, feature_split_info, feature_gini_indicies\n",
    "\n",
    "# Calculates gain ratio\n",
    "def calc_gain_ratio(info_gains, split_info):\n",
    "    if len(info_gains) != len(split_info):\n",
    "        print(\"ERROR\")\n",
    "    else:\n",
    "        feature_gain_ratio = []\n",
    "        for i in range(len(info_gains)):\n",
    "            feature_gain_ratio.append(info_gains[i]/split_info[i])\n",
    "            \n",
    "    return feature_gain_ratio\n",
    "\n",
    "# Combines indicies with tuples\n",
    "def tup_index_val_list(info):\n",
    "    info_tups = []\n",
    "    for e in range(len(info)):\n",
    "        tup = (e, info[e])\n",
    "        info_tups.append(tup)\n",
    "    return info_tups\n",
    "\n",
    "# Prints info on which columnss are good or bad for splitting\n",
    "def output(df, info_gains, gain_ratios, gini_indicies):\n",
    "    print(\"Best splits\")\n",
    "    max_info_gain = info_gains.index(max(info_gains))\n",
    "    max_gain_ratio = gain_ratios.index(max(gain_ratios))\n",
    "    min_gini_index = gini_indicies.index(min(gini_indicies))\n",
    "    \n",
    "    print(df.columns[max_info_gain])\n",
    "    print(df.columns[max_gain_ratio])\n",
    "    print(df.columns[min_gini_index])\n",
    "    \n",
    "    print()\n",
    "    print(\"Worst Splits\")\n",
    "    min_info_gain = info_gains.index(min(info_gains))\n",
    "    min_gain_ratio = gain_ratios.index(min(gain_ratios))\n",
    "    max_gini_index = gini_indicies.index(max(gini_indicies))\n",
    "    \n",
    "    print(df.columns[min_info_gain])\n",
    "    print(df.columns[min_gain_ratio])\n",
    "    print(df.columns[max_gini_index])\n",
    "\n",
    "    print()\n",
    "    print(\"Sorted\")\n",
    "    info_gain_tups = tup_index_val_list(info_gains)\n",
    "    gain_ratio_tups = tup_index_val_list(gain_ratios)\n",
    "    gini_index_tups = tup_index_val_list(gini_indicies)\n",
    "    \n",
    "    info_gain_tups.sort(key=lambda tup: -tup[1])\n",
    "    gain_ratio_tups.sort(key=lambda tup: -tup[1])\n",
    "    gini_index_tups.sort(key=lambda tup: tup[1])\n",
    "    \n",
    "    print(\"Top 5\")\n",
    "    for i in range(5):\n",
    "        print()\n",
    "        print(df.columns[info_gain_tups[i][0]], info_gain_tups[i][1])\n",
    "        print(df.columns[gain_ratio_tups[i][0]], gain_ratio_tups[i][1])\n",
    "        print(df.columns[gini_index_tups[i][0]], gini_index_tups[i][1])\n",
    "    \n",
    "    print()\n",
    "    print(\"Bottom 5\")\n",
    "    for i in range(1, 11):\n",
    "        print()\n",
    "        print(df.columns[info_gain_tups[-i][0]], info_gain_tups[-i][1])\n",
    "        print(df.columns[gain_ratio_tups[-i][0]], gain_ratio_tups[-i][1])\n",
    "        print(df.columns[gini_index_tups[-i][0]], gini_index_tups[-i][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Engineeering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainlib = pd.read_csv(\"training.csv\")\n",
    "testlib = pd.read_csv(\"testing.csv\")\n",
    "\n",
    "# Combine libraries for data processing\n",
    "all_data = trainlib.append(testlib)\n",
    "\n",
    "# Split Product Info 2\n",
    "all_data['Product_Info_2_char'] = all_data.Product_Info_2.str[0]\n",
    "all_data['Product_Info_2_num'] = all_data.Product_Info_2.str[1]\n",
    "\n",
    "\n",
    "categorical_cols =[\"Product_Info_1\",\"Product_Info_2\",\"Product_Info_3\",\"Product_Info_5\",\"Product_Info_6\",\"Product_Info_7\",\"Employment_Info_2\",\"Employment_Info_3\",\"Employment_Info_5\",\"InsuredInfo_1\",\"InsuredInfo_2\",\"InsuredInfo_3\",\"InsuredInfo_4\",\"InsuredInfo_5\",\"InsuredInfo_6\",\"InsuredInfo_7\",\"Insurance_History_1\",\"Insurance_History_2\",\"Insurance_History_3\",\"Insurance_History_4\",\"Insurance_History_7\",\"Insurance_History_8\",\"Insurance_History_9\",\"Family_Hist_1\",\"Medical_History_2\",\"Medical_History_3\",\"Medical_History_4\",\"Medical_History_5\",\"Medical_History_6\",\"Medical_History_7\",\"Medical_History_8\",\"Medical_History_9\",\"Medical_History_11\",\"Medical_History_12\",\"Medical_History_13\",\"Medical_History_14\",\"Medical_History_16\",\"Medical_History_17\",\"Medical_History_18\",\"Medical_History_19\",\"Medical_History_20\",\"Medical_History_21\",\"Medical_History_22\",\"Medical_History_23\",\"Medical_History_25\",\"Medical_History_26\",\"Medical_History_27\",\"Medical_History_28\",\"Medical_History_29\",\"Medical_History_30\",\"Medical_History_31\",\"Medical_History_33\",\"Medical_History_34\",\"Medical_History_35\",\"Medical_History_36\",\"Medical_History_37\",\"Medical_History_38\",\"Medical_History_39\",\"Medical_History_40\",\"Medical_History_41\",\"Product_Info_2_char\",\"Product_Info_2_num\"]\n",
    "for i in categorical_cols:\n",
    "    all_data[i] = pd.factorize(all_data[i])[0]\n",
    "\n",
    "# Add BMI_Age\n",
    "all_data['BMI_Age'] = all_data['BMI'] * all_data['Ins_Age']\n",
    "\n",
    "# Drop unnecessary columns\n",
    "all_data.drop(['Product_Info_2', 'Medical_Keyword_44', 'Medical_Keyword_45'], axis=1, inplace=True)\n",
    "\n",
    "# Add Med Keyword Count\n",
    "med_keyword_columns = all_data.columns[all_data.columns.str.startswith('Medical_Keyword_')]\n",
    "all_data['Med_Keywords_Count'] = all_data[med_keyword_columns].sum(axis=1)\n",
    "\n",
    "# Fill N/A with 0\n",
    "for key in med_keyword_columns:\n",
    "    all_data[key].fillna(0, inplace=True)\n",
    "all_data['countna'] = all_data.apply(lambda x: sum(x.isnull()),1)\n",
    "\n",
    "# Replace N/A with mean/median/mode\n",
    "columns = all_data.columns\n",
    "for col in columns:\n",
    "    if col not in med_keyword_columns and col != 'Response':\n",
    "        #fill = np.mode(all_data(col))\n",
    "        fill = all_data[col].median()\n",
    "        #fill = np.mean(all_data[col])\n",
    "        all_data[col].fillna(fill, inplace=True)\n",
    "all_data.fillna(0, inplace=True)\n",
    "\n",
    "all_data['Response'] = all_data['Response'].astype(int)\n",
    "\n",
    "train_ohd = all_data[all_data['Response']>0].copy()\n",
    "test_ohd = all_data[all_data['Response']<1].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set response column as last column in df\n",
    "cols = list(train_ohd.columns.values) #Make a list of all of the columns in the df\n",
    "cols.pop(cols.index('Response')) #Remove response from list\n",
    "train_ohd = train_ohd[cols+['Response']]\n",
    "\n",
    "# Edit me if you want to adjust drops for entire db\n",
    "drop_cols = [\"Id\", \"Response\",\"Product_Info_1\",\"Product_Info_3\",\"Product_Info_5\",\"Product_Info_6\",\"Product_Info_7\",\"Employment_Info_2\",\"Employment_Info_3\",\"Employment_Info_5\",\"InsuredInfo_1\",\"InsuredInfo_2\",\"InsuredInfo_3\",\"InsuredInfo_4\",\"InsuredInfo_5\",\"InsuredInfo_6\",\"InsuredInfo_7\",\"Insurance_History_1\",\"Insurance_History_2\",\"Insurance_History_3\",\"Insurance_History_4\",\"Insurance_History_7\",\"Insurance_History_8\",\"Insurance_History_9\",\"Family_Hist_1\",\"Medical_History_2\",\"Medical_History_3\",\"Medical_History_4\",\"Medical_History_5\",\"Medical_History_6\",\"Medical_History_7\",\"Medical_History_8\",\"Medical_History_9\", \"Medical_History_11\",\"Medical_History_12\",\"Medical_History_13\",\"Medical_History_14\",\"Medical_History_16\",\"Medical_History_17\",\"Medical_History_18\",\"Medical_History_19\",\"Medical_History_20\",\"Medical_History_21\",\"Medical_History_22\",\"Medical_History_23\",\"Medical_History_25\",\"Medical_History_26\",\"Medical_History_27\",\"Medical_History_28\",\"Medical_History_29\",\"Medical_History_30\",\"Medical_History_31\",\"Medical_History_33\",\"Medical_History_34\",\"Medical_History_35\",\"Medical_History_36\",\"Medical_History_37\",\"Medical_History_38\",\"Medical_History_39\",\"Medical_History_40\",\"Medical_History_41\"]\n",
    "\n",
    "# Extract wanted columns\n",
    "def partition_columns(cols):\n",
    "    return train_ohd[cols], test_ohd[cols]\n",
    "\n",
    "# Separate out the response column\n",
    "target = train_ohd[\"Response\"]\n",
    "train_db = train_ohd.drop(drop_cols, axis=1)\n",
    "test_db = test_ohd.drop(drop_cols, axis=1)\n",
    "\n",
    "# Edit me if you want a subsection\n",
    "#rand_db, rand_tdb = partition_columns(['Product_Info_2_char', 'Product_Info_2_num', 'BMI_Age', 'Med_Keywords_Count', 'countna', 'Wt', 'Ht'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run HW 5 on dataset\n",
    "n = len(target)\n",
    "info_gains, split_info, gini_indicies = get_info_gains(n, train_ohd)\n",
    "gain_ratios = calc_gain_ratio(info_gains, split_info)\n",
    "\n",
    "# Uncomment next line for outputs\n",
    "#output(train_ohd, info_gains, gain_ratios, gini_indicies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 569, 2: 474, 3: 113, 4: 351, 5: 560, 6: 1510, 7: 900, 8: 5523})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SKLearn Logistic Regression For Comparison Purposes\n",
    "from sklearn import datasets, linear_model\n",
    "\n",
    "# Create linear regression object\n",
    "regr = linear_model.LogisticRegression(C=1e5)\n",
    "\n",
    "# Train the model using the training sets\n",
    "#regr.fit(train_db, target)\n",
    "regr.fit(train_db, target)\n",
    "\n",
    "# Make predictions using the testing set\n",
    "#new_pred = regr.predict(test_db)\n",
    "new_pred = regr.predict(test_db)\n",
    "Counter(new_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Counter({1: 576, 2: 484, 3: 112, 4: 352, 5: 553, 6: 1524, 7: 852, 8: 5547})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data setup\n",
    "#rand_data = rand_db.append(rand_tdb) # For subsection\n",
    "rand_data = train_db.append(test_db) # For full data\n",
    "\n",
    "Classes = [1,2,3,4,5,6,7,8]\n",
    "\n",
    "#Number of examples\n",
    "m = rand_data.shape[0]\n",
    "#Features\n",
    "n = rand_data.shape[1]\n",
    "#Number of classes\n",
    "k = 8\n",
    "\n",
    "# Initialize data\n",
    "X = np.ones((m,n + 1))\n",
    "y = np.array((m,1))\n",
    "cols = rand_data.columns\n",
    "for col in range(len(cols)):\n",
    "    X[:,col+1] = rand_data[cols[col]].values\n",
    "    \n",
    "#Labels\n",
    "y = target.as_matrix()\n",
    "\n",
    "# Split data\n",
    "X_train = X[:20000]\n",
    "y_train = y\n",
    "X_test = X[20000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code from https://www.kaggle.com/anthonysegura/logistic-regression-from-scratch\n",
    "\n",
    "# Logistic Regression\n",
    "def sigmoid(z):\n",
    "    return 1.0 / (1 + np.exp(-z))\n",
    "\n",
    "# Regularized cost function\n",
    "def regCostFunction(theta, X, y, _lambda = 0.1):\n",
    "    m = len(y)\n",
    "    h = sigmoid(X.dot(theta))\n",
    "    tmp = np.copy(theta)\n",
    "    tmp[0] = 0 \n",
    "    reg = (_lambda/(2*m)) * np.sum(tmp**2)\n",
    "\n",
    "    return (1 / m) * (-y.T.dot(np.log(h)) - (1 - y).T.dot(np.log(1 - h))) + reg\n",
    "\n",
    "# Regularized gradient function\n",
    "def regGradient(theta, X, y, _lambda = 0.1):\n",
    "    m, n = X.shape\n",
    "    theta = theta.reshape((n, 1))\n",
    "    y = y.reshape((m, 1))\n",
    "    h = sigmoid(X.dot(theta))\n",
    "    tmp = np.copy(theta)\n",
    "    tmp[0] = 0\n",
    "    reg = _lambda*tmp /m\n",
    "\n",
    "    return ((1 / m) * X.T.dot(h - y)) + reg\n",
    "\n",
    "# Optimal theta \n",
    "def logisticRegression(X, y, theta):\n",
    "    result = op.minimize(fun = regCostFunction, x0 = theta, args = (X, y), method = 'TNC', jac = regGradient)\n",
    "    \n",
    "    return result.x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_theta = np.zeros((k, n + 1))\n",
    "\n",
    "#One vs all\n",
    "i = 0\n",
    "for iter in range(8):\n",
    "    # set the labels in 0 and 1\n",
    "    tmp_y = np.array(y_train == iter+1, dtype = int)\n",
    "    optTheta = logisticRegression(X_train, tmp_y, np.zeros((n + 1,1)))\n",
    "    all_theta[i] = optTheta\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 571, 2: 462, 3: 111, 4: 350, 5: 552, 6: 1536, 7: 886, 8: 5532})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict on dataset\n",
    "P = sigmoid(X_test.dot(all_theta.T)) #probability for each flower\n",
    "p = [Classes[np.argmax(P[i, :])] for i in range(X_test.shape[0])]\n",
    "LR_preds = p\n",
    "Counter(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission csv\n",
    "preds = LR_preds\n",
    "res = pd.DataFrame()\n",
    "id_arr = [x for x in range(20000, len(preds)+20000)]\n",
    "res[\"Id\"] = id_arr\n",
    "res.set_index(\"Id\")\n",
    "res[\"Response\"] = preds\n",
    "res.to_csv(\"predictions.csv\", index=False)\n",
    "temp = pd.read_csv(\"predictions.csv\")\n",
    "#print(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2-Layer DNN - Fails with large loss from cross entropy.\n",
    "\n",
    "- Auto Encoder needed\n",
    "- Gradient Vanishing\n",
    "- Same issue with 3 and 4 Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running MiniBatchGCD with 10 epochs\n",
      "\n",
      "\n",
      "Epoch 0 Loss 5.787531791949748 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:52: RuntimeWarning: divide by zero encountered in log\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:71: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F[i] [ -811.89429817  -839.29143229 -1210.23859613 -1039.4727789\n",
      " -1079.10467893 -1334.27039058  -896.04716755  -793.30033417]\n",
      "F[i] [ -811.89429817  -839.29143229 -1210.23859613 -1039.4727789\n",
      " -1079.10467893 -1334.27039058  -896.04716755  -793.30033417]\n",
      "F[i] [ -811.89429817  -839.29143229 -1210.23859613 -1039.4727789\n",
      " -1079.10467893 -1334.27039058  -896.04716755  -793.30033417]\n",
      "F[i] [ -811.89429817  -839.29143229 -1210.23859613 -1039.4727789\n",
      " -1079.10467893 -1334.27039058  -896.04716755  -793.30033417]\n",
      "F[i] [ -811.89429817  -839.29143229 -1210.23859613 -1039.4727789\n",
      " -1079.10467893 -1334.27039058  -896.04716755  -793.30033417]\n",
      "F[i] [ -811.89429817  -839.29143229 -1210.23859613 -1039.4727789\n",
      " -1079.10467893 -1334.27039058  -896.04716755  -793.30033417]\n",
      "F[i] [ -811.89429817  -839.29143229 -1210.23859613 -1039.4727789\n",
      " -1079.10467893 -1334.27039058  -896.04716755  -793.30033417]\n",
      "F[i] [ -811.89429817  -839.29143229 -1210.23859613 -1039.4727789\n",
      " -1079.10467893 -1334.27039058  -896.04716755  -793.30033417]\n",
      "F[i] [ -810.22765535  -837.5685491  -1207.7542389  -1037.33896683\n",
      " -1076.88951117 -1331.53142299  -894.20777715  -791.67186072]\n",
      "F[i] [ -810.22765535  -837.5685491  -1207.7542389  -1037.33896683\n",
      " -1076.88951117 -1331.53142299  -894.20777715  -791.67186072]\n",
      "F[i] [ -810.22765535  -837.5685491  -1207.7542389  -1037.33896683\n",
      " -1076.88951117 -1331.53142299  -894.20777715  -791.67186072]\n",
      "F[i] [ -810.22765535  -837.5685491  -1207.7542389  -1037.33896683\n",
      " -1076.88951117 -1331.53142299  -894.20777715  -791.67186072]\n",
      "F[i] [ -810.22765535  -837.5685491  -1207.7542389  -1037.33896683\n",
      " -1076.88951117 -1331.53142299  -894.20777715  -791.67186072]\n",
      "F[i] [ -810.22765535  -837.5685491  -1207.7542389  -1037.33896683\n",
      " -1076.88951117 -1331.53142299  -894.20777715  -791.67186072]\n",
      "F[i] [ -810.22765535  -837.5685491  -1207.7542389  -1037.33896683\n",
      " -1076.88951117 -1331.53142299  -894.20777715  -791.67186072]\n",
      "F[i] [ -810.22765535  -837.5685491  -1207.7542389  -1037.33896683\n",
      " -1076.88951117 -1331.53142299  -894.20777715  -791.67186072]\n",
      "F[i] [ -812.95435652  -840.38726199 -1211.8187569  -1040.82997748\n",
      " -1080.51362332 -1336.01249477  -897.21710098  -794.33611511]\n",
      "F[i] [ -812.95435652  -840.38726199 -1211.8187569  -1040.82997748\n",
      " -1080.51362332 -1336.01249477  -897.21710098  -794.33611511]\n",
      "F[i] [ -812.95435652  -840.38726199 -1211.8187569  -1040.82997748\n",
      " -1080.51362332 -1336.01249477  -897.21710098  -794.33611511]\n",
      "F[i] [ -812.95435652  -840.38726199 -1211.8187569  -1040.82997748\n",
      " -1080.51362332 -1336.01249477  -897.21710098  -794.33611511]\n",
      "F[i] [ -812.95435652  -840.38726199 -1211.8187569  -1040.82997748\n",
      " -1080.51362332 -1336.01249477  -897.21710098  -794.33611511]\n",
      "F[i] [ -812.95435652  -840.38726199 -1211.8187569  -1040.82997748\n",
      " -1080.51362332 -1336.01249477  -897.21710098  -794.33611511]\n",
      "F[i] [ -812.95435652  -840.38726199 -1211.8187569  -1040.82997748\n",
      " -1080.51362332 -1336.01249477  -897.21710098  -794.33611511]\n",
      "F[i] [ -812.95435652  -840.38726199 -1211.8187569  -1040.82997748\n",
      " -1080.51362332 -1336.01249477  -897.21710098  -794.33611511]\n",
      "F[i] [ -811.54586568  -838.93124204 -1209.71921022 -1039.02667884\n",
      " -1078.64157044 -1333.6977752   -895.66262001  -792.95988146]\n",
      "F[i] [ -811.54586568  -838.93124204 -1209.71921022 -1039.02667884\n",
      " -1078.64157044 -1333.6977752   -895.66262001  -792.95988146]\n",
      "F[i] [ -811.54586568  -838.93124204 -1209.71921022 -1039.02667884\n",
      " -1078.64157044 -1333.6977752   -895.66262001  -792.95988146]\n",
      "F[i] [ -811.54586568  -838.93124204 -1209.71921022 -1039.02667884\n",
      " -1078.64157044 -1333.6977752   -895.66262001  -792.95988146]\n",
      "F[i] [ -811.54586568  -838.93124204 -1209.71921022 -1039.02667884\n",
      " -1078.64157044 -1333.6977752   -895.66262001  -792.95988146]\n",
      "F[i] [ -811.54586568  -838.93124204 -1209.71921022 -1039.02667884\n",
      " -1078.64157044 -1333.6977752   -895.66262001  -792.95988146]\n",
      "F[i] [ -811.54586568  -838.93124204 -1209.71921022 -1039.02667884\n",
      " -1078.64157044 -1333.6977752   -895.66262001  -792.95988146]\n",
      "F[i] [ -811.54586568  -838.93124204 -1209.71921022 -1039.02667884\n",
      " -1078.64157044 -1333.6977752   -895.66262001  -792.95988146]\n",
      "F[i] [ -862.20146584  -891.29620052 -1285.2282544  -1103.88132505\n",
      " -1145.96892485 -1416.94539446  -951.56867472  -842.45536952]\n",
      "F[i] [ -862.20146584  -891.29620052 -1285.2282544  -1103.88132505\n",
      " -1145.96892485 -1416.94539446  -951.56867472  -842.45536952]\n",
      "F[i] [ -862.20146584  -891.29620052 -1285.2282544  -1103.88132505\n",
      " -1145.96892485 -1416.94539446  -951.56867472  -842.45536952]\n",
      "F[i] [ -862.20146584  -891.29620052 -1285.2282544  -1103.88132505\n",
      " -1145.96892485 -1416.94539446  -951.56867472  -842.45536952]\n",
      "F[i] [ -862.20146584  -891.29620052 -1285.2282544  -1103.88132505\n",
      " -1145.96892485 -1416.94539446  -951.56867472  -842.45536952]\n",
      "F[i] [ -862.20146584  -891.29620052 -1285.2282544  -1103.88132505\n",
      " -1145.96892485 -1416.94539446  -951.56867472  -842.45536952]\n",
      "F[i] [ -862.20146584  -891.29620052 -1285.2282544  -1103.88132505\n",
      " -1145.96892485 -1416.94539446  -951.56867472  -842.45536952]\n",
      "F[i] [ -862.20146584  -891.29620052 -1285.2282544  -1103.88132505\n",
      " -1145.96892485 -1416.94539446  -951.56867472  -842.45536952]\n",
      "\n",
      "Epoch 1 Loss nan \n",
      "\n",
      "\n",
      "Epoch 2 Loss nan \n",
      "\n",
      "\n",
      "Epoch 3 Loss nan \n",
      "\n",
      "\n",
      "Epoch 4 Loss nan \n",
      "\n",
      "\n",
      "Epoch 5 Loss nan \n",
      "\n",
      "\n",
      "Epoch 6 Loss nan \n",
      "\n",
      "\n",
      "Epoch 7 Loss nan \n",
      "\n",
      "\n",
      "Epoch 8 Loss nan \n",
      "\n",
      "\n",
      "Epoch 9 Loss nan \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:42: RuntimeWarning: invalid value encountered in greater\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 13.25\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Computes Matrix Multiplication\n",
    "# Returns Z = A x W + b, cache\n",
    "# A - nxd / n -> num of training data points\n",
    "# W - dxd'/ d -> num of i/p features\n",
    "# b - d'  / d'-> num of o/p features\n",
    "# Z - nxd'\n",
    "def Affine_Forward(A,W,b):\n",
    "    Z = np.matmul(A,W)\n",
    "    Z = Z+np.array([b]*A.shape[0])\n",
    "    return Z,(A,W,b)\n",
    "\n",
    "def Affine_Backwards(dZ,cache):\n",
    "    A=cache[0]\n",
    "    W=cache[1]\n",
    "    b=cache[2]\n",
    "\n",
    "    dA = np.zeros(A.shape)\n",
    "    dW = np.zeros(W.shape)\n",
    "    db = np.zeros(b.shape)\n",
    "\n",
    "    W_T = W.transpose()\n",
    "    dA=np.matmul(dZ,W_T)\n",
    "\n",
    "    A_T = A.transpose()\n",
    "    dW = np.matmul(A_T,dZ)\n",
    "\n",
    "    for j in range(b.shape[0]):\n",
    "        db[j]+=sum(dZ[:,j])\n",
    "\n",
    "    return dA,dW,db\n",
    "\n",
    "def ReLU_Forward(Z):\n",
    "    Z_cache = np.array(Z)\n",
    "    return np.maximum(Z,0,Z),Z_cache\n",
    "\n",
    "def ReLU_Backward(dA,Z_cache):\n",
    "    dZ = np.zeros(dA.shape)\n",
    "    dZ = np.where(Z_cache>0.0,dA,0)\n",
    "    return dZ\n",
    "\n",
    "def Cross_Entropy(F,y):\n",
    "    L_sub = 0\n",
    "    n = F.shape[0]\n",
    "\n",
    "    #print(\"y\", y)\n",
    "    for i in range(n):\n",
    "        #print(\"F[i]\",i, F[i])\n",
    "        L_sub += F[i,int(y[i]-1)] - np.log(sum(np.exp(F[i])))\n",
    "\n",
    "    L = -1.0/n * L_sub\n",
    "\n",
    "    dF = np.zeros(F.shape)\n",
    "\n",
    "    for i in range(F.shape[0]):\n",
    "        for j in range(F.shape[1]):\n",
    "            if(j==int(y[i])):\n",
    "                match=1\n",
    "            else:\n",
    "                match=0\n",
    "            if n ==0:\n",
    "                print(\"N\")\n",
    "            if sum(np.exp(F[i])) == 0:\n",
    "                #print(\"F\", F)\n",
    "                print(\"F[i]\", F[i])\n",
    "                #print(\"exp\", np.exp(F[i]))\n",
    "            \n",
    "            dF[i,j] = -1.0/n*(match-(np.exp(F[i,j])/sum(np.exp(F[i]))))\n",
    "    return L,dF\n",
    "\n",
    "#Learning Rate\n",
    "eta = 0.1\n",
    "#NeuronCount at Layer1,2,3\n",
    "Neurons=[64,8]\n",
    "#Batch Size\n",
    "b_size = 100\n",
    "#Weights\n",
    "W1 = []\n",
    "W2 = []\n",
    "#bias\n",
    "b1 = []\n",
    "b2 = []\n",
    "\n",
    "#Four Layer Neural Net\n",
    "#X      - input batch of n datapoints with d observations each\n",
    "#W_all  - Weights for all 4 layers\n",
    "#b_all  - bias for all 4 layers\n",
    "#y      - training labels/correct actions\n",
    "#test   - select Train/Test mode for network\n",
    "def FourLvlNN(X,y,test,min_loss):\n",
    "    global W1\n",
    "    global W2\n",
    "    \n",
    "    global b1\n",
    "    global b2\n",
    "\n",
    "    Z1,cache_a1 = Affine_Forward(X,W1,b1)\n",
    "    #print(\"Z1\", Z1)\n",
    "    #print(\"X\", X)\n",
    "    A1,cache_r1 = ReLU_Forward(Z1)\n",
    "    #print(\"A1\", A1)\n",
    "    F,cache_a2  = Affine_Forward(A1,W2,b2)\n",
    "    #print(\"F\", F)\n",
    "\n",
    "    if(test==True):\n",
    "        classification = np.zeros((F.shape[0],))\n",
    "        classification = np.argmax(F,axis=1)\n",
    "        return classification\n",
    "    \n",
    "    loss,dF     = Cross_Entropy(F,y)\n",
    "    #print(\"L\", loss)\n",
    "    #print(\"dF\", dF)\n",
    "    dA1,dW2,db2 = Affine_Backwards(dF,cache_a2)\n",
    "    dZ1         = ReLU_Backward(dA1,cache_r1)\n",
    "    #print(\"dZ1\", dZ1)\n",
    "    dX,dW1,db1  = Affine_Backwards(dZ1,cache_a1)\n",
    "\n",
    "    if(loss<min_loss):\n",
    "        min_loss=loss\n",
    "        np.save(\"Weights.npy\",np.asarray([W1,W2]))\n",
    "        np.save(\"Bias.npy\",np.asarray([b1,b2]))\n",
    "\n",
    "    #Gradient Descent\n",
    "    W1 = W1 - eta*dW1\n",
    "    W2 = W2 - eta*dW2\n",
    "    #print(\"1\", dW1)\n",
    "    #print(\"2\", dW2)\n",
    "    #print(\"3\", dW3)\n",
    "\n",
    "    return loss\n",
    "\n",
    "def load_DataSet():\n",
    "    data=np.loadtxt(\"expert_policy.txt\")\n",
    "    return data\n",
    "\n",
    "#Initialize weights and biases\n",
    "def bAndW_init(num_ip_features):\n",
    "    global W1\n",
    "    global W2\n",
    "    \n",
    "    global b1\n",
    "    global b2\n",
    "\n",
    "    W1 = np.random.uniform(0,0.8,(num_ip_features,Neurons[0]))\n",
    "    W2 = np.random.uniform(0,0.8,(Neurons[0],Neurons[1]))\n",
    "    \n",
    "    b1 = np.zeros((Neurons[0],))\n",
    "    b2 = np.zeros((Neurons[1],))\n",
    "'''\n",
    "All data in dataset already normalized\n",
    "def normalize(X):\n",
    "    data = X.copy()\n",
    "    for feature in data:\n",
    "        mean = np.mean(feature)\n",
    "        std = np.std(feature)\n",
    "        feature -= mean\n",
    "        feature /= std\n",
    "    #print(\"D\",data)\n",
    "    #print(\"X\",X)\n",
    "    return data\n",
    "\n",
    "def normalize_dataset(data):\n",
    "    for feature in range(len(data)-1):\n",
    "        mean = np.mean(data[feature])\n",
    "        std = np.std(data[feature])\n",
    "        data[feature] -= mean\n",
    "        data[feature] /= std\n",
    "    return data\n",
    "'''\n",
    "def MiniBatchGD(data,epoch):\n",
    "    global W1\n",
    "    global W2\n",
    "    \n",
    "    global b1\n",
    "    global b2\n",
    "    print(\"Running MiniBatchGCD with\",epoch,\"epochs\\n\")\n",
    "    num_ip_features = data.shape[1]-1\n",
    "    #Global score for 'b's and 'W's\n",
    "    bAndW_init(num_ip_features)\n",
    "\n",
    "    loss_list=[]\n",
    "    min_loss = 100\n",
    "\n",
    "    for cycle in range(epoch):\n",
    "        np.random.shuffle(data)\n",
    "        for i in range(data.shape[0]//b_size):\n",
    "            #displayProgess(i,data.shape[0]//b_size)\n",
    "            X = data[i*b_size:(i+1)*b_size,:-1]\n",
    "            #print(\"X\",X)\n",
    "            #X = normalize(X)\n",
    "            y = data[i*b_size:(i+1)*b_size,-1]\n",
    "            loss = FourLvlNN(X,y,False,min_loss)\n",
    "        print(\"\\nEpoch\",cycle,\"Loss\",loss,\"\\n\")\n",
    "        loss_list.append(loss)\n",
    "\n",
    "    np.save(\"LossCurve.npy\",np.asarray(loss_list))\n",
    "\n",
    "def evaluateAccuracy(data):\n",
    "    global W1\n",
    "    global W2\n",
    "    \n",
    "    global b1\n",
    "    global b2\n",
    "\n",
    "    W_all = np.load(\"Weights.npy\")\n",
    "    b_all = np.load(\"Bias.npy\")\n",
    "    W1 = W_all[0]\n",
    "    W2 = W_all[1]\n",
    "    b1 = b_all[0]\n",
    "    b2 = b_all[1]\n",
    "\n",
    "    X = data[:,:-1]\n",
    "    y = data[:,-1]\n",
    "    classifications =FourLvlNN(X,y,True,-1)\n",
    "    correct=0\n",
    "    for i in range(len(y)):\n",
    "        if(classifications[i]==int(y[i])):\n",
    "            correct+=1\n",
    "    accuracy = correct/len(y)*100\n",
    "    print(\"Testing Accuracy:\",accuracy)\n",
    "\n",
    "def displayLossCurve():\n",
    "    loss_array=np.load(\"LossCurve.npy\")\n",
    "    plt.plot(loss_array)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.show()\n",
    "\n",
    "def displayProgess(i,N):\n",
    "    sys.stdout.write('\\r')\n",
    "    sys.stdout.write(\"Batch %d/%d in progress\"%(i+1,N))\n",
    "    bars = int((i/N*100)//10)\n",
    "    sys.stdout.write(\" [%-10s]\"%('='*bars+'>',))\n",
    "\n",
    "def main():\n",
    "    nn_db = pd.concat([rand_db, target], axis=1)\n",
    "    data = nn_db.as_matrix()\n",
    "    MiniBatchGD(data,10)\n",
    "    displayLossCurve()\n",
    "    evaluateAccuracy(data)\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
